{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert-loves-chemistry'...\n",
      "remote: Enumerating objects: 1566, done.\u001b[K\n",
      "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
      "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
      "remote: Total 1566 (delta 96), reused 92 (delta 91), pack-reused 1364\u001b[K\n",
      "Receiving objects: 100% (1566/1566), 55.35 MiB | 14.27 MiB/s, done.\n",
      "Resolving deltas: 100% (1000/1000), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinnunes/Virtual Laboratories for Pharmaceutical/vlab/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
    "from bertviz import head_view\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\", output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'C1=CC=<mask>=C1'\n",
    "tokenized = tokenizer.encode(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer.encode(smiles, return_tensors=\"pt\")\n",
    "output = model(encoded_input)\n",
    "logits = output.logits[0,tokenized.index(4),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(262)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(262)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', '<mask>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9755934476852417, 'token': 33, 'token_str': '=', 'sequence': 'C1=CC=CC=C1'}\n",
      "{'score': 0.020923908799886703, 'token': 7, 'token_str': '#', 'sequence': 'C1=CC=CC#C1'}\n",
      "{'score': 0.0007658947724848986, 'token': 21, 'token_str': '1', 'sequence': 'C1=CC=CC1C1'}\n",
      "{'score': 0.00041297602001577616, 'token': 22, 'token_str': '2', 'sequence': 'C1=CC=CC2C1'}\n",
      "{'score': 0.00025319133419543505, 'token': 352, 'token_str': '=[', 'sequence': 'C1=CC=CC=[C1'}\n"
     ]
    }
   ],
   "source": [
    "smiles_mask = \"C1=CC=CC<mask>C1\"\n",
    "smiles = \"C1=CC=CC=C1\"\n",
    "\n",
    "masked_smi = fill_mask(smiles_mask)\n",
    "\n",
    "for smi in masked_smi:\n",
    "  print(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.6558, -14.0462,  -7.4074,  ..., -15.2723, -14.8948, -15.3795],\n",
       "         [ -7.4523, -11.3929,  -8.8325,  ..., -12.4926, -12.0528, -11.8053],\n",
       "         [ -7.0814, -13.6186,  -7.5875,  ..., -14.8810, -14.5286, -15.0764]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=(tensor([[[-0.9260,  1.0885, -1.4870,  ...,  1.4126,  0.0499, -0.0160],\n",
       "         [-0.3525,  0.7792,  0.5669,  ...,  0.0285,  1.4401, -0.3127],\n",
       "         [-0.4993,  0.2187,  1.3723,  ...,  0.8860,  1.8355, -0.6392]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.7085,  1.2924, -1.1331,  ...,  0.8577,  0.1635, -0.2324],\n",
       "         [-0.5091,  1.3939,  1.1030,  ..., -0.3351,  2.3625, -0.6673],\n",
       "         [-1.6204,  0.2253,  0.8990,  ...,  1.1815,  2.7231, -0.9860]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.0472,  0.7284, -0.7592,  ...,  1.0540,  0.6680,  1.3268],\n",
       "         [-0.4530,  1.4330,  1.2938,  ..., -0.7182,  1.8009,  1.1720],\n",
       "         [-2.3590,  0.3752,  0.4402,  ...,  1.1037,  2.0294,  0.7446]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3267,  0.3497, -0.3461,  ...,  0.4470,  0.2643, -0.0402],\n",
       "         [ 0.0975,  1.9213,  1.1964,  ..., -0.2391,  1.3419,  0.5980],\n",
       "         [-1.3127, -0.0773, -0.4122,  ...,  0.4949,  0.7737,  0.1472]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3990, -0.0805,  0.1915,  ..., -0.4645,  0.9612, -0.4551],\n",
       "         [ 0.3306,  1.0263,  1.0993,  ..., -1.0803,  1.7034, -0.7934],\n",
       "         [-1.3765, -0.4385, -0.0872,  ..., -0.8545,  1.1561, -0.2714]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.4647, -0.8179, -0.3109,  ..., -0.9235,  1.3605, -1.1053],\n",
       "         [ 0.5348, -0.4094, -0.4389,  ..., -1.8492,  1.2896, -2.1175],\n",
       "         [-1.4001, -1.5211, -0.5652,  ..., -1.2904,  1.2634, -0.5061]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.6688, -2.7352,  0.1691,  ..., -1.4119,  1.2160, -0.6919],\n",
       "         [-0.7885, -2.1138,  0.1580,  ..., -0.5649,  1.6685, -1.0881],\n",
       "         [-1.9996, -2.9164,  0.0997,  ..., -1.3669,  1.0870, -0.5613]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), attentions=(tensor([[[[4.4201e-01, 3.7420e-01, 1.8379e-01],\n",
       "          [3.2134e-01, 8.6871e-02, 5.9179e-01],\n",
       "          [4.6991e-01, 3.1015e-02, 4.9907e-01]],\n",
       "\n",
       "         [[1.7787e-01, 4.3593e-01, 3.8621e-01],\n",
       "          [7.1337e-01, 1.5948e-01, 1.2716e-01],\n",
       "          [7.7723e-01, 1.5884e-01, 6.3922e-02]],\n",
       "\n",
       "         [[1.0540e-01, 6.9304e-01, 2.0156e-01],\n",
       "          [4.6540e-01, 3.2028e-01, 2.1432e-01],\n",
       "          [2.3270e-01, 6.2837e-01, 1.3893e-01]],\n",
       "\n",
       "         [[3.4360e-01, 2.4610e-01, 4.1030e-01],\n",
       "          [1.7952e-01, 5.9219e-02, 7.6126e-01],\n",
       "          [8.5551e-02, 3.7252e-02, 8.7720e-01]],\n",
       "\n",
       "         [[4.9809e-01, 2.3814e-01, 2.6378e-01],\n",
       "          [9.8086e-03, 9.4301e-01, 4.7186e-02],\n",
       "          [8.4414e-04, 9.9308e-01, 6.0801e-03]],\n",
       "\n",
       "         [[6.7239e-01, 1.1639e-01, 2.1121e-01],\n",
       "          [2.9935e-01, 4.6968e-01, 2.3097e-01],\n",
       "          [9.7310e-02, 1.6064e-01, 7.4205e-01]],\n",
       "\n",
       "         [[3.4458e-01, 4.3336e-01, 2.2206e-01],\n",
       "          [5.7527e-01, 3.1377e-01, 1.1096e-01],\n",
       "          [8.8397e-03, 7.9528e-01, 1.9588e-01]],\n",
       "\n",
       "         [[5.4787e-01, 2.3707e-01, 2.1506e-01],\n",
       "          [4.0010e-01, 2.8609e-01, 3.1380e-01],\n",
       "          [9.8140e-02, 6.3136e-01, 2.7050e-01]],\n",
       "\n",
       "         [[5.1915e-01, 3.7104e-01, 1.0980e-01],\n",
       "          [5.1185e-01, 3.6838e-01, 1.1977e-01],\n",
       "          [2.9270e-01, 5.6141e-01, 1.4589e-01]],\n",
       "\n",
       "         [[9.1217e-01, 5.4988e-02, 3.2837e-02],\n",
       "          [3.2446e-04, 1.4032e-01, 8.5935e-01],\n",
       "          [7.8480e-04, 9.3379e-01, 6.5429e-02]],\n",
       "\n",
       "         [[6.0406e-01, 1.9281e-01, 2.0313e-01],\n",
       "          [1.0778e-01, 3.3639e-01, 5.5583e-01],\n",
       "          [5.9856e-02, 6.5478e-01, 2.8536e-01]],\n",
       "\n",
       "         [[3.7293e-01, 6.7144e-02, 5.5993e-01],\n",
       "          [1.1629e-02, 3.0230e-03, 9.8535e-01],\n",
       "          [3.5667e-02, 2.7968e-02, 9.3636e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.9804e-01, 4.4323e-01, 2.5873e-01],\n",
       "          [4.2488e-01, 3.6655e-01, 2.0857e-01],\n",
       "          [2.9693e-01, 5.9236e-01, 1.1071e-01]],\n",
       "\n",
       "         [[1.4306e-01, 6.6655e-01, 1.9040e-01],\n",
       "          [5.4498e-02, 6.6870e-01, 2.7680e-01],\n",
       "          [9.7775e-03, 6.7109e-01, 3.1913e-01]],\n",
       "\n",
       "         [[2.2991e-01, 6.0366e-01, 1.6643e-01],\n",
       "          [8.9861e-01, 4.5883e-02, 5.5511e-02],\n",
       "          [1.9415e-01, 7.3619e-01, 6.9660e-02]],\n",
       "\n",
       "         [[9.6912e-01, 1.9301e-02, 1.1577e-02],\n",
       "          [1.7409e-02, 3.2588e-01, 6.5671e-01],\n",
       "          [4.1695e-03, 2.5677e-01, 7.3906e-01]],\n",
       "\n",
       "         [[3.7576e-01, 3.5849e-01, 2.6576e-01],\n",
       "          [1.8912e-02, 7.6195e-01, 2.1914e-01],\n",
       "          [9.7895e-02, 4.9810e-01, 4.0401e-01]],\n",
       "\n",
       "         [[7.7931e-01, 1.8381e-01, 3.6879e-02],\n",
       "          [3.2834e-02, 1.2391e-01, 8.4326e-01],\n",
       "          [1.4773e-01, 5.3607e-01, 3.1620e-01]],\n",
       "\n",
       "         [[8.1233e-02, 8.6143e-01, 5.7336e-02],\n",
       "          [2.6489e-01, 3.5773e-01, 3.7738e-01],\n",
       "          [1.0149e-02, 9.8540e-01, 4.4528e-03]],\n",
       "\n",
       "         [[9.9802e-01, 5.6240e-04, 1.4137e-03],\n",
       "          [5.9745e-01, 7.4525e-02, 3.2803e-01],\n",
       "          [9.7702e-03, 7.9821e-01, 1.9202e-01]],\n",
       "\n",
       "         [[3.9135e-01, 2.3298e-01, 3.7567e-01],\n",
       "          [3.8323e-03, 7.2102e-01, 2.7515e-01],\n",
       "          [4.8117e-03, 2.7592e-01, 7.1927e-01]],\n",
       "\n",
       "         [[1.4147e-01, 8.3754e-01, 2.0993e-02],\n",
       "          [4.2758e-01, 1.0230e-01, 4.7011e-01],\n",
       "          [1.7178e-01, 2.7143e-01, 5.5679e-01]],\n",
       "\n",
       "         [[8.6046e-01, 5.0835e-02, 8.8703e-02],\n",
       "          [5.1423e-01, 1.7983e-01, 3.0594e-01],\n",
       "          [2.5643e-02, 4.4190e-01, 5.3245e-01]],\n",
       "\n",
       "         [[8.8559e-01, 2.3239e-02, 9.1167e-02],\n",
       "          [8.4626e-01, 2.2718e-02, 1.3102e-01],\n",
       "          [9.1218e-01, 3.7181e-02, 5.0638e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.7057e-02, 3.3713e-01, 5.9581e-01],\n",
       "          [4.4148e-02, 3.2661e-01, 6.2924e-01],\n",
       "          [1.8459e-03, 2.3869e-01, 7.5946e-01]],\n",
       "\n",
       "         [[9.6575e-01, 2.6813e-02, 7.4392e-03],\n",
       "          [9.2108e-01, 3.8828e-02, 4.0095e-02],\n",
       "          [2.4351e-02, 9.3737e-01, 3.8279e-02]],\n",
       "\n",
       "         [[9.9076e-01, 2.4144e-03, 6.8280e-03],\n",
       "          [9.2726e-01, 1.8209e-03, 7.0919e-02],\n",
       "          [6.1545e-01, 2.5676e-01, 1.2779e-01]],\n",
       "\n",
       "         [[2.9974e-01, 6.4132e-01, 5.8941e-02],\n",
       "          [5.5955e-02, 7.7598e-01, 1.6806e-01],\n",
       "          [1.0289e-03, 9.9510e-01, 3.8694e-03]],\n",
       "\n",
       "         [[8.0001e-01, 1.7408e-01, 2.5912e-02],\n",
       "          [2.3616e-01, 6.6240e-01, 1.0144e-01],\n",
       "          [1.5340e-02, 9.1925e-01, 6.5414e-02]],\n",
       "\n",
       "         [[3.4721e-01, 2.1261e-01, 4.4018e-01],\n",
       "          [4.8635e-01, 1.4608e-01, 3.6757e-01],\n",
       "          [2.9306e-01, 4.3798e-02, 6.6314e-01]],\n",
       "\n",
       "         [[2.1445e-01, 1.3417e-01, 6.5137e-01],\n",
       "          [3.4085e-04, 6.3327e-04, 9.9903e-01],\n",
       "          [2.2661e-03, 1.2312e-01, 8.7462e-01]],\n",
       "\n",
       "         [[8.7699e-01, 9.8964e-02, 2.4048e-02],\n",
       "          [6.9999e-02, 3.3453e-02, 8.9655e-01],\n",
       "          [4.9759e-02, 9.3818e-01, 1.2063e-02]],\n",
       "\n",
       "         [[4.6530e-01, 2.1604e-01, 3.1866e-01],\n",
       "          [4.3409e-03, 8.7661e-02, 9.0800e-01],\n",
       "          [2.0109e-03, 2.3550e-01, 7.6249e-01]],\n",
       "\n",
       "         [[9.9944e-01, 2.2576e-04, 3.3578e-04],\n",
       "          [1.9789e-02, 9.2968e-01, 5.0530e-02],\n",
       "          [2.1734e-01, 7.1700e-01, 6.5665e-02]],\n",
       "\n",
       "         [[9.3890e-02, 7.2496e-01, 1.8115e-01],\n",
       "          [7.4824e-03, 8.8145e-01, 1.1107e-01],\n",
       "          [2.1964e-02, 6.7275e-01, 3.0529e-01]],\n",
       "\n",
       "         [[5.6902e-01, 3.4432e-01, 8.6657e-02],\n",
       "          [7.2250e-02, 3.1418e-01, 6.1357e-01],\n",
       "          [2.1149e-02, 9.2661e-01, 5.2238e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.5641, 0.1730, 0.2629],\n",
       "          [0.2154, 0.5940, 0.1906],\n",
       "          [0.4061, 0.3159, 0.2781]],\n",
       "\n",
       "         [[0.5038, 0.3163, 0.1799],\n",
       "          [0.1272, 0.6276, 0.2451],\n",
       "          [0.1707, 0.6765, 0.1528]],\n",
       "\n",
       "         [[0.7850, 0.1002, 0.1148],\n",
       "          [0.2451, 0.5476, 0.2073],\n",
       "          [0.4414, 0.3553, 0.2033]],\n",
       "\n",
       "         [[0.2987, 0.4319, 0.2694],\n",
       "          [0.2089, 0.5487, 0.2425],\n",
       "          [0.3931, 0.3031, 0.3038]],\n",
       "\n",
       "         [[0.2996, 0.5270, 0.1734],\n",
       "          [0.0276, 0.9219, 0.0505],\n",
       "          [0.1128, 0.7940, 0.0931]],\n",
       "\n",
       "         [[0.0963, 0.8813, 0.0224],\n",
       "          [0.0272, 0.9476, 0.0252],\n",
       "          [0.0677, 0.9179, 0.0143]],\n",
       "\n",
       "         [[0.7531, 0.0713, 0.1756],\n",
       "          [0.0798, 0.7010, 0.2192],\n",
       "          [0.4453, 0.2661, 0.2886]],\n",
       "\n",
       "         [[0.5449, 0.4035, 0.0516],\n",
       "          [0.3693, 0.5154, 0.1153],\n",
       "          [0.3790, 0.5754, 0.0456]],\n",
       "\n",
       "         [[0.1733, 0.7216, 0.1051],\n",
       "          [0.1414, 0.1700, 0.6887],\n",
       "          [0.0588, 0.8659, 0.0753]],\n",
       "\n",
       "         [[0.4683, 0.4410, 0.0907],\n",
       "          [0.1901, 0.3969, 0.4130],\n",
       "          [0.1080, 0.8609, 0.0310]],\n",
       "\n",
       "         [[0.0595, 0.8753, 0.0653],\n",
       "          [0.0206, 0.9595, 0.0199],\n",
       "          [0.0347, 0.9152, 0.0501]],\n",
       "\n",
       "         [[0.3050, 0.4364, 0.2587],\n",
       "          [0.2663, 0.3597, 0.3740],\n",
       "          [0.4264, 0.2225, 0.3511]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.3147, 0.3491, 0.3361],\n",
       "          [0.2165, 0.6123, 0.1711],\n",
       "          [0.4453, 0.2748, 0.2799]],\n",
       "\n",
       "         [[0.1792, 0.4915, 0.3293],\n",
       "          [0.1731, 0.6058, 0.2210],\n",
       "          [0.2377, 0.4475, 0.3148]],\n",
       "\n",
       "         [[0.5113, 0.2042, 0.2845],\n",
       "          [0.1102, 0.6247, 0.2651],\n",
       "          [0.3289, 0.4070, 0.2641]],\n",
       "\n",
       "         [[0.4208, 0.3086, 0.2706],\n",
       "          [0.2008, 0.3704, 0.4288],\n",
       "          [0.3168, 0.3808, 0.3024]],\n",
       "\n",
       "         [[0.3383, 0.3603, 0.3013],\n",
       "          [0.2750, 0.3854, 0.3397],\n",
       "          [0.2533, 0.4777, 0.2690]],\n",
       "\n",
       "         [[0.4486, 0.1969, 0.3545],\n",
       "          [0.2423, 0.4962, 0.2615],\n",
       "          [0.4098, 0.2568, 0.3333]],\n",
       "\n",
       "         [[0.1437, 0.7387, 0.1176],\n",
       "          [0.0672, 0.8618, 0.0710],\n",
       "          [0.1104, 0.7756, 0.1140]],\n",
       "\n",
       "         [[0.2823, 0.3192, 0.3985],\n",
       "          [0.3819, 0.2035, 0.4146],\n",
       "          [0.2196, 0.4644, 0.3160]],\n",
       "\n",
       "         [[0.0997, 0.7976, 0.1027],\n",
       "          [0.0332, 0.9243, 0.0425],\n",
       "          [0.0805, 0.8243, 0.0953]],\n",
       "\n",
       "         [[0.2097, 0.4464, 0.3440],\n",
       "          [0.1662, 0.3849, 0.4490],\n",
       "          [0.1278, 0.5795, 0.2926]],\n",
       "\n",
       "         [[0.1007, 0.7597, 0.1396],\n",
       "          [0.0437, 0.8320, 0.1243],\n",
       "          [0.1448, 0.6851, 0.1701]],\n",
       "\n",
       "         [[0.3455, 0.2202, 0.4343],\n",
       "          [0.2398, 0.3102, 0.4499],\n",
       "          [0.3191, 0.2246, 0.4563]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.3262, 0.3593, 0.3145],\n",
       "          [0.2205, 0.3712, 0.4083],\n",
       "          [0.3662, 0.2725, 0.3612]],\n",
       "\n",
       "         [[0.2655, 0.4283, 0.3062],\n",
       "          [0.2992, 0.2723, 0.4286],\n",
       "          [0.2759, 0.3564, 0.3677]],\n",
       "\n",
       "         [[0.2853, 0.3300, 0.3847],\n",
       "          [0.1995, 0.3576, 0.4429],\n",
       "          [0.2616, 0.3899, 0.3485]],\n",
       "\n",
       "         [[0.2338, 0.5029, 0.2633],\n",
       "          [0.2046, 0.4855, 0.3099],\n",
       "          [0.2193, 0.5845, 0.1962]],\n",
       "\n",
       "         [[0.0866, 0.5901, 0.3233],\n",
       "          [0.0964, 0.6227, 0.2809],\n",
       "          [0.0948, 0.6903, 0.2148]],\n",
       "\n",
       "         [[0.0303, 0.9309, 0.0387],\n",
       "          [0.0728, 0.8378, 0.0894],\n",
       "          [0.0473, 0.8664, 0.0862]],\n",
       "\n",
       "         [[0.1831, 0.2750, 0.5419],\n",
       "          [0.2916, 0.0908, 0.6176],\n",
       "          [0.1433, 0.3005, 0.5562]],\n",
       "\n",
       "         [[0.2701, 0.5517, 0.1781],\n",
       "          [0.2560, 0.5151, 0.2288],\n",
       "          [0.3049, 0.5228, 0.1723]],\n",
       "\n",
       "         [[0.1747, 0.6818, 0.1435],\n",
       "          [0.1830, 0.6661, 0.1509],\n",
       "          [0.1224, 0.7696, 0.1080]],\n",
       "\n",
       "         [[0.3246, 0.3126, 0.3628],\n",
       "          [0.2513, 0.5070, 0.2418],\n",
       "          [0.3591, 0.2184, 0.4225]],\n",
       "\n",
       "         [[0.3608, 0.2485, 0.3907],\n",
       "          [0.1545, 0.4690, 0.3765],\n",
       "          [0.2871, 0.2662, 0.4467]],\n",
       "\n",
       "         [[0.1716, 0.6326, 0.1958],\n",
       "          [0.1105, 0.6626, 0.2269],\n",
       "          [0.1870, 0.5799, 0.2331]]]], grad_fn=<SoftmaxBackward0>)))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles = \"CCO\"\n",
    "encoded_input = tokenizer.encode(smiles, return_tensors=\"pt\")\n",
    "output = model(encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 324,   2]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 324,   2]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#O=C(OCC)C\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>CCO</s>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 52000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 7924\n",
      "Model config: RobertaConfig {\n",
      "  \"_name_or_path\": \"seyonec/PubChem10M_SMILES_BPE_450k\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model config: {model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "cos(output.logits[0][10],output.logits[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -7.6558, -14.0462,  -7.4074,  ..., -15.2723, -14.8948, -15.3795])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0][0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinnunes/Virtual Laboratories for Pharmaceutical/vlab/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Softmax()\n",
    "s = m(output.logits[0][0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCO'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Softmax()\n",
    "s = m(output.logits[0][1].detach())\n",
    "tokenizer.decode(torch.tensor([s.argmax()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3945e-08, 5.1417e-09, 1.0764e-08, 8.4116e-09, 5.9243e-09, 1.1958e-08,\n",
       "        1.8460e-08, 1.2109e-08, 6.8984e-09, 6.0611e-09, 1.2832e-08, 1.3336e-08,\n",
       "        5.8392e-09, 4.8676e-09, 6.2577e-09, 1.4798e-08, 6.9734e-09, 8.5084e-09,\n",
       "        7.6099e-09, 7.6932e-09, 6.5979e-09, 5.2883e-09, 1.2863e-08, 7.8229e-09,\n",
       "        4.2298e-09, 1.0856e-08, 1.0040e-08, 9.6652e-09, 7.5489e-09, 1.2422e-08,\n",
       "        4.8839e-09, 1.4095e-08, 1.8170e-08, 4.8783e-09, 9.7936e-09, 1.1018e-08,\n",
       "        6.2736e-09, 4.8806e-09, 5.7192e-09, 8.1560e-09, 5.9114e-09, 5.2537e-09,\n",
       "        6.9231e-09, 8.0058e-09, 1.5052e-08, 9.0575e-09, 1.4172e-08, 6.8955e-09,\n",
       "        1.3890e-08, 1.1775e-08, 5.3803e-09, 1.8080e-08, 6.1643e-09, 7.7689e-09,\n",
       "        1.2203e-08, 5.3980e-09, 9.4877e-09, 1.0576e-08, 1.0425e-08, 9.5385e-09,\n",
       "        9.3665e-09, 1.2337e-08, 1.1493e-08, 8.0305e-09, 1.3615e-08, 1.1460e-08,\n",
       "        5.6252e-09, 5.5747e-09, 4.3661e-09, 1.4847e-08, 9.8487e-09, 7.0689e-09,\n",
       "        1.0863e-08, 6.5225e-09, 1.0949e-08, 1.1257e-08, 7.3337e-09, 1.0684e-08,\n",
       "        8.3839e-09, 8.6344e-09, 8.1081e-09, 7.8354e-09, 9.4947e-09, 1.5902e-08,\n",
       "        6.0411e-09, 9.3062e-09, 1.0046e-08, 5.0398e-09, 7.8420e-09, 8.4646e-09,\n",
       "        1.5600e-08, 9.4731e-09, 5.3034e-09, 8.1746e-09, 5.8665e-09, 1.0133e-08,\n",
       "        7.8041e-09, 4.4504e-09, 6.9090e-09, 8.8495e-09])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8766e-07, 1.3366e-08, 1.7296e-07, 1.3566e-06, 3.0956e-07, 1.3600e-06,\n",
       "        1.9463e-06, 9.0404e-04, 1.7684e-06, 1.5250e-05, 2.1486e-06, 8.9456e-07,\n",
       "        2.5004e-05, 3.3969e-03, 7.1444e-07, 1.0598e-04, 1.0077e-06, 4.5854e-05,\n",
       "        1.3879e-06, 5.3039e-06, 3.3418e-06, 4.2262e-03, 2.1702e-05, 1.6995e-03,\n",
       "        6.1067e-04, 1.0590e-03, 1.0454e-04, 4.0084e-04, 4.6368e-06, 1.2190e-04,\n",
       "        6.7041e-05, 1.7494e-06, 1.1451e-06, 1.5270e-04, 3.6365e-06, 1.6737e-06,\n",
       "        3.9430e-06, 3.0914e-06, 1.0548e-04, 7.5182e-05, 1.1254e-05, 1.4388e-06,\n",
       "        9.3995e-05, 3.1024e-06, 1.4391e-06, 5.2804e-03, 2.2665e-06, 3.1863e-06,\n",
       "        1.1972e-06, 9.6301e-07, 2.1825e-04, 2.9617e-04, 6.0474e-04, 7.1826e-07,\n",
       "        5.9401e-07, 1.4375e-04, 1.0582e-05, 1.1215e-06, 4.8120e-05, 1.4863e-04,\n",
       "        1.7824e-06, 1.2336e-06, 4.1196e-06, 7.5092e-05, 1.1030e-06, 4.6979e-02,\n",
       "        2.5599e-06, 2.1039e-06, 9.0069e-07, 1.4240e-06, 2.9356e-06, 2.0591e-04,\n",
       "        5.6854e-07, 5.2945e-06, 8.7724e-06, 1.8026e-06, 1.0339e-06, 1.7981e-06,\n",
       "        6.1151e-07, 3.0032e-06, 8.3665e-06, 3.4474e-06, 2.8872e-03, 3.2589e-03,\n",
       "        1.0682e-03, 1.5200e-06, 6.7013e-06, 3.1738e-03, 9.4512e-06, 1.7170e-06,\n",
       "        1.6641e-06, 1.5093e-06, 5.0170e-05, 2.6905e-06, 1.9766e-06, 5.8553e-05,\n",
       "        1.8647e-06, 5.1884e-05, 2.4820e-06, 7.6431e-07])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[7900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethanol = 'C'\n",
    "ethanol_input = tokenizer.encode(ethanol, return_tensors='pt')\n",
    "ethanol = model(ethanol_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 52000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethanol.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "methanol = 'CO'\n",
    "methanol_input = tokenizer.encode(methanol, return_tensors='pt')\n",
    "methanol = model(methanol_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.5177, -13.6238,  -8.3360,  ..., -14.6676, -14.1055, -14.3423],\n",
       "         [ -8.2455,  -9.1575,  -8.5876,  ...,  -9.5612,  -8.8451,  -8.7961],\n",
       "         [ -7.5205, -12.2304,  -8.2294,  ..., -13.2789, -12.7507, -12.9583]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methanol.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizerFast(name_or_path='seyonec/PubChem10M_SMILES_BPE_450k', vocab_size=7924, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.7612, -11.6706,  -8.3844,  ..., -12.5026, -11.9004, -12.0322]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9972, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "cos(methanol.logits[0].mean(axis=1),ethanol.logits[0].mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -7.7461, -10.9860,  -8.6876,  ..., -11.3977, -10.7377, -10.8192],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinnunes/Virtual Laboratories for Pharmaceutical/vlab/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\", output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='seyonec/PubChem10M_SMILES_BPE_450k', vocab_size=7924, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_  = 'C1=CC=C(C=C1)I.B(C1=CC=CC=C1)(O)O.CO'\n",
    "output = tokenizer.encode(input_, return_tensors='pt')\n",
    "encoded = model(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  39,  21,  33, 262,  33,  39,  12,  39,  33,  39,  21,  13,  45,\n",
       "          18,  38,  12,  39,  21,  33, 262,  33, 262,  33,  39,  21, 274,  51,\n",
       "          13,  51,  18, 298,   2]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
